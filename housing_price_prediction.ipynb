{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29c3727",
   "metadata": {},
   "source": [
    "**Note**: Make sure the `hdfs` package is installed in your Jupyter environment. We've installed it using:\n",
    "```bash\n",
    "pip install hdfs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e832a",
   "metadata": {},
   "source": [
    "# Housing Price Prediction with PySpark and HDFS\n",
    "\n",
    "This notebook demonstrates an end-to-end machine learning pipeline using PySpark and HDFS for housing price prediction. We'll cover:\n",
    "\n",
    "1. Setting up PySpark with Jupyter\n",
    "2. Working with HDFS\n",
    "3. Data preparation and feature engineering\n",
    "4. Training a Linear Regression model\n",
    "5. Model evaluation\n",
    "6. Model persistence\n",
    "7. Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525cfe2",
   "metadata": {},
   "source": [
    "## 1. Setup PySpark with Jupyter\n",
    "\n",
    "First, let's initialize our PySpark session and verify the setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f170867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark WebUI URL: http://ff41454abd16:4040\n",
      "Spark Application ID: app-20251026153120-0000\n",
      "Available cores: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HousingPricePrediction\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-spark-1:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test the connection\n",
    "print(\"Spark WebUI URL:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"Spark Application ID:\", spark.sparkContext.applicationId)\n",
    "print(\"Available cores:\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f12cd",
   "metadata": {},
   "source": [
    "## 2. Connect to HDFS and Load Data\n",
    "\n",
    "Now we'll download the housing dataset and upload it to HDFS. We'll use the `hdfs` python library to interact with HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0daa37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to HDFS successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Initialize HDFS client\n",
    "hdfs_client = InsecureClient('http://namenode:9870')\n",
    "\n",
    "# First, let's create our HDFS directory structure\n",
    "hdfs_base_path = '/user/spark/housing_data'\n",
    "try:\n",
    "    hdfs_client.makedirs(hdfs_base_path)\n",
    "except Exception as e:\n",
    "    print(f\"Directory might already exist: {e}\")\n",
    "\n",
    "# For this example, we'll use a sample of the housing dataset\n",
    "# In practice, you would download from Kaggle using their API\n",
    "sample_data = {\n",
    "    'price': [100000, 200000, 150000, 300000],\n",
    "    'bedrooms': [2, 3, 2, 4],\n",
    "    'bathrooms': [1, 2, 1, 3],\n",
    "    'sqft_living': [1000, 1500, 1200, 2000],\n",
    "    'sqft_lot': [2000, 3000, 2500, 4000],\n",
    "    'floors': [1, 2, 1, 2],\n",
    "    'waterfront': [0, 0, 1, 0],\n",
    "    'condition': [3, 4, 3, 5]\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Save to a temporary CSV file\n",
    "temp_file = 'temp_housing.csv'\n",
    "df.to_csv(temp_file, index=False)\n",
    "\n",
    "# Upload to HDFS using the correct method\n",
    "hdfs_path = hdfs_base_path + '/housing.csv'\n",
    "hdfs_client.upload(hdfs_path, temp_file)\n",
    "\n",
    "# Clean up temporary file\n",
    "os.remove(temp_file)\n",
    "\n",
    "print(\"Data uploaded to HDFS successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef098b",
   "metadata": {},
   "source": [
    "## 3. Prepare Housing Price Dataset\n",
    "\n",
    "Now let's load the data from HDFS into a PySpark DataFrame and prepare it for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e32e65f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Read data from HDFS\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mspark\u001b[49m.read.csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhdfs://namenode:9000\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhdfs_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/housing.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Display schema and sample data\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataFrame Schema:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Read data from HDFS\n",
    "df = spark.read.csv(f\"hdfs://namenode:9000{hdfs_base_path}/housing.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display schema and sample data\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nSample Data:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Select features for the model\n",
    "feature_columns = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'condition']\n",
    "\n",
    "# Create feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Select features and label for modeling\n",
    "data_for_ml = data.select(\"features\", col(\"price\").alias(\"label\"))\n",
    "\n",
    "print(\"Prepared data sample:\")\n",
    "data_for_ml.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3f0cd",
   "metadata": {},
   "source": [
    "## 4. Build Linear Regression Model\n",
    "\n",
    "Now we'll split our data into training and testing sets, and train a Linear Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data, test_data = data_for_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Import and create the model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    regParam=0.3,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b8efb",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Predictions\n",
    "\n",
    "Let's evaluate our model's performance on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select prediction and label columns\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Import evaluation metrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Calculate RMSE and R2\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE) = {rmse}\")\n",
    "print(f\"RÂ² = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda88d60",
   "metadata": {},
   "source": [
    "## 6. Save and Load Model\n",
    "\n",
    "We'll save our trained model to HDFS for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1318b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to HDFS\n",
    "model_path = f\"hdfs://namenode:9000{hdfs_base_path}/housing_model\"\n",
    "model.save(model_path)\n",
    "\n",
    "# Load the model back (for demonstration)\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "loaded_model = LinearRegressionModel.load(model_path)\n",
    "\n",
    "print(\"Model successfully saved and loaded from HDFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552696f",
   "metadata": {},
   "source": [
    "## 7. Batch Inference from HDFS\n",
    "\n",
    "Finally, let's demonstrate how to use our saved model for batch inference on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new sample data for prediction\n",
    "new_houses = spark.createDataFrame([\n",
    "    (3, 2, 1500, 3000, 1, 0, 4),  # House 1\n",
    "    (4, 3, 2200, 4000, 2, 1, 5),  # House 2\n",
    "], feature_columns)\n",
    "\n",
    "# Prepare features\n",
    "new_data = assembler.transform(new_houses)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.transform(new_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(feature_columns + [\"prediction\"]).show()\n",
    "\n",
    "# Save predictions back to HDFS\n",
    "predictions_path = f\"hdfs://namenode:9000{hdfs_base_path}/predictions\"\n",
    "predictions.write.mode(\"overwrite\").csv(predictions_path)\n",
    "\n",
    "print(\"Predictions saved to HDFS at:\", predictions_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241a248",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Finally, let's clean up our Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
